{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript\n",
        "\n",
        "def keep_alive():\n",
        "    display(Javascript('''\n",
        "        function keepAlive() {\n",
        "            console.log(\"Keeping Colab alive...\");\n",
        "            setTimeout(keepAlive, 60000);\n",
        "        }\n",
        "        keepAlive();\n",
        "    '''))\n",
        "\n",
        "keep_alive()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "R4IVoYoefEVV",
        "outputId": "56f66748-4933-44c9-9480-0616084e1996"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        function keepAlive() {\n",
              "            console.log(\"Keeping Colab alive...\");\n",
              "            setTimeout(keepAlive, 60000);\n",
              "        }\n",
              "        keepAlive();\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium beautifulsoup4 pandas webdriver-manager fake-useragent\n",
        "!apt-get update\n",
        "!apt-get install -y chromium-browser\n",
        "!apt install chromium-chromedriver\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!dpkg -i google-chrome-stable_current_amd64.deb\n",
        "!apt-get -f install -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv3PBO4XH-1p",
        "outputId": "f6c77498-1ac5-47b9-a72b-0569ffb43bba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (4.28.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: webdriver-manager in /usr/local/lib/python3.11/dist-packages (4.0.2)\n",
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.11/dist-packages (2.0.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.28.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2024.12.14)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (2.32.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (1.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver-manager) (3.4.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Get:1 https://dl.google.com/linux/chrome/deb stable InRelease [1,825 B]\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:9 https://dl.google.com/linux/chrome/deb stable/main amd64 Packages [1,211 B]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 3,036 B in 2s (1,473 B/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "chromium-browser is already the newest version (1:85.0.4183.83-0ubuntu2.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (1:85.0.4183.83-0ubuntu2.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n",
            "--2025-01-29 17:39:30--  https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
            "Resolving dl.google.com (dl.google.com)... 74.125.26.91, 74.125.26.190, 74.125.26.93, ...\n",
            "Connecting to dl.google.com (dl.google.com)|74.125.26.91|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 112524488 (107M) [application/x-debian-package]\n",
            "Saving to: ‘google-chrome-stable_current_amd64.deb.1’\n",
            "\n",
            "google-chrome-stabl 100%[===================>] 107.31M   322MB/s    in 0.3s    \n",
            "\n",
            "2025-01-29 17:39:30 (322 MB/s) - ‘google-chrome-stable_current_amd64.deb.1’ saved [112524488/112524488]\n",
            "\n",
            "(Reading database ... 125570 files and directories currently installed.)\n",
            "Preparing to unpack google-chrome-stable_current_amd64.deb ...\n",
            "Unpacking google-chrome-stable (132.0.6834.159-1) over (132.0.6834.159-1) ...\n",
            "Setting up google-chrome-stable (132.0.6834.159-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "0 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "#import csv\n",
        "#from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "#from tenacity import retry, stop_after_attempt, wait_exponential"
      ],
      "metadata": {
        "id": "i7RkeJm1IIZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get a random user-agent\n",
        "def get_random_user_agent():\n",
        "    ua = UserAgent()\n",
        "    return ua.random\n",
        "\n",
        "# Setup WebDriver with enhanced options\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "chrome_options.add_argument(f\"user-agent={get_random_user_agent()}\")\n",
        "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "chrome_options.add_experimental_option(\"useAutomationExtension\", False)\n",
        "\n",
        "service = Service(ChromeDriverManager().install())\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "driver.set_page_load_timeout(60)  # Set page load timeout to 60 seconds\n",
        "\n",
        "# Base URL\n",
        "base_url = \"https://www.the-numbers.com\"\n",
        "budget_url = f\"{base_url}/movie/budgets/all\"\n",
        "page_numbers = list(range(101, 1001, 100))  # Generate page numbers [1, 101, ..., 6501]\n",
        "\n",
        "# Initialize an empty DataFrame\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# Function to extract table data\n",
        "def extract_table_data(table):\n",
        "    data = {}\n",
        "    if table:\n",
        "        rows = table.find_all(\"tr\")\n",
        "        for row in rows:\n",
        "            cols = row.find_all(\"td\")\n",
        "            if len(cols) == 2:\n",
        "                key = cols[0].text.strip()\n",
        "                value = cols[1].text.strip()\n",
        "                data[key] = value\n",
        "    return data\n",
        "\n",
        "# Function to scrape extra movie details\n",
        "def scrape_movie_details(movie_url):\n",
        "    try:\n",
        "        driver.get(movie_url)\n",
        "        # Wait for the page to load\n",
        "        WebDriverWait(driver, 20).until(\n",
        "            EC.presence_of_element_located((By.CSS_SELECTOR, \"#movie_finances\"))\n",
        "        )\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "        details = {\"Movie URL\": movie_url}\n",
        "\n",
        "        # Extract financial information\n",
        "        financial_table = soup.find(\"table\", id=\"movie_finances\")\n",
        "        if financial_table:\n",
        "            rows = financial_table.find_all(\"tr\")\n",
        "            for row in rows:\n",
        "                cols = row.find_all(\"td\")\n",
        "                if len(cols) >= 2:  # Ensure it's a key-value row\n",
        "                    key = cols[0].get_text(strip=True)\n",
        "                    value = cols[1].get_text(strip=True)\n",
        "                    details[key] = value\n",
        "\n",
        "        # Extract metrics\n",
        "        metrics_table = soup.select_one(\"#summary > table:nth-of-type(1)\")\n",
        "        if metrics_table:\n",
        "            rows = metrics_table.find_all(\"tr\")\n",
        "            for row in rows:\n",
        "                cols = row.find_all(\"td\")\n",
        "                if len(cols) == 2:\n",
        "                    key = cols[0].get_text(strip=True)\n",
        "                    value = cols[1].get_text(strip=True)\n",
        "                    details[key] = value\n",
        "\n",
        "        # Extract movie details\n",
        "        details_table = soup.select_one(\"#summary > table:nth-of-type(3)\")\n",
        "        if not details_table:\n",
        "            # If the first table is not found, look for the second table\n",
        "            details_table = soup.select_one(\"#summary_mobile > div > table:nth-child(11)\")\n",
        "\n",
        "        if details_table:\n",
        "            rows = details_table.find_all(\"tr\")\n",
        "            for row in rows:\n",
        "                cols = row.find_all(\"td\")\n",
        "                if len(cols) == 2:\n",
        "                    key = cols[0].get_text(strip=True)\n",
        "                    value = cols[1].get_text(strip=True)\n",
        "                    details[key] = value\n",
        "\n",
        "        return details\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {movie_url}: {e}\")\n",
        "        return {}\n",
        "\n",
        "# Loop through all budget pages\n",
        "for page in page_numbers:\n",
        "    url = f\"{budget_url}/{page}\" if page != 1 else budget_url\n",
        "    print(f\"Scraping: {url}\")\n",
        "\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        # Wait for the table to load\n",
        "        WebDriverWait(driver, 20).until(\n",
        "            EC.presence_of_element_located((By.TAG_NAME, \"table\"))\n",
        "        )\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "        table = soup.find(\"table\")\n",
        "\n",
        "        # Extract headers (only once)\n",
        "        if df.empty:\n",
        "            headers = [header.text.strip() for header in table.find_all(\"th\")] + [\"Movie URL\"]\n",
        "\n",
        "        # Extract table rows\n",
        "        for row in table.find_all(\"tr\")[1:]:  # Skip header row\n",
        "            cells = row.find_all(\"td\")\n",
        "            row_data = [cell.text.strip() for cell in cells]\n",
        "\n",
        "            # Find movie link inside <b> <a> tag\n",
        "            movie_link_tag = row.find(\"b\").find(\"a\") if row.find(\"b\") else None\n",
        "            movie_link = f\"{base_url}{movie_link_tag['href']}\" if movie_link_tag else \"N/A\"\n",
        "\n",
        "            if row_data:\n",
        "                row_data.append(movie_link)  # Append movie URL\n",
        "\n",
        "                # Scrape extra movie details\n",
        "                if movie_link != \"N/A\":\n",
        "                    extra_details = scrape_movie_details(movie_link)\n",
        "                    extra_details.pop(\"Movie URL\", None)\n",
        "                    row_data.extend(extra_details.values())  # Append extra details dynamically\n",
        "\n",
        "                # Create a temporary DataFrame for the current movie\n",
        "                temp_df = pd.DataFrame([row_data], columns=headers + list(extra_details.keys()))\n",
        "\n",
        "                # Append the temporary DataFrame to the main DataFrame\n",
        "                df = pd.concat([df, temp_df], ignore_index=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "        continue\n",
        "\n",
        "# Close browser\n",
        "driver.quit()\n",
        "\n",
        "# Save to CSV\n",
        "csv_path = \"movie_budgets_with_details.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(f\"Scraping completed! CSV file saved: {csv_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_UIjeNj0GE7",
        "outputId": "9cfdbc8a-c917-41a1-d651-5af04609aaae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: https://www.the-numbers.com/movie/budgets/all/101\n"
          ]
        }
      ]
    }
  ]
}