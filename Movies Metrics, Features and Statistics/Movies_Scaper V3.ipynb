{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript\n",
        "\n",
        "def keep_alive():\n",
        "    display(Javascript('''\n",
        "        function keepAlive() {\n",
        "            console.log(\"Keeping Colab alive...\");\n",
        "            setTimeout(keepAlive, 60000);\n",
        "        }\n",
        "        keepAlive();\n",
        "    '''))\n",
        "\n",
        "keep_alive()"
      ],
      "metadata": {
        "id": "R4IVoYoefEVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium beautifulsoup4 pandas webdriver-manager fake-useragent\n",
        "!apt-get update\n",
        "!apt-get install -y chromium-browser\n",
        "!apt install chromium-chromedriver\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!dpkg -i google-chrome-stable_current_amd64.deb\n",
        "!apt-get -f install -y"
      ],
      "metadata": {
        "id": "Mv3PBO4XH-1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "#import csv\n",
        "#from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "#from tenacity import retry, stop_after_attempt, wait_exponential"
      ],
      "metadata": {
        "id": "i7RkeJm1IIZJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get a random user-agent\n",
        "def get_random_user_agent():\n",
        "    ua = UserAgent()\n",
        "    return ua.random\n",
        "\n",
        "# Setup WebDriver with enhanced options\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "chrome_options.add_argument(f\"user-agent={get_random_user_agent()}\")\n",
        "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "chrome_options.add_experimental_option(\"useAutomationExtension\", False)\n",
        "\n",
        "service = Service(ChromeDriverManager().install())\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "driver.set_page_load_timeout(60)  # Set page load timeout to 60 seconds\n",
        "\n",
        "# Base URL\n",
        "base_url = \"https://www.the-numbers.com\"\n",
        "budget_url = f\"{base_url}/movie/budgets/all\"\n",
        "page_numbers = list(range(1, 6601, 100))  # Generate page numbers [1, 101, ..., 6501]\n",
        "\n",
        "# Initialize an empty DataFrame\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# Function to extract table data\n",
        "def extract_table_data(table):\n",
        "    data = {}\n",
        "    if table:\n",
        "        rows = table.find_all(\"tr\")\n",
        "        for row in rows:\n",
        "            cols = row.find_all(\"td\")\n",
        "            if len(cols) == 2:\n",
        "                key = cols[0].text.strip()\n",
        "                value = cols[1].text.strip()\n",
        "                data[key] = value\n",
        "    return data\n",
        "\n",
        "# Function to scrape extra movie details\n",
        "def scrape_movie_details(movie_url):\n",
        "    try:\n",
        "        driver.get(movie_url)\n",
        "        # Wait for the page to load\n",
        "        WebDriverWait(driver, 20).until(\n",
        "            EC.presence_of_element_located((By.CSS_SELECTOR, \"#movie_finances\"))\n",
        "        )\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "        details = {\"Movie URL\": movie_url}\n",
        "\n",
        "        # Extract financial information\n",
        "        financial_table = soup.find(\"table\", id=\"movie_finances\")\n",
        "        if financial_table:\n",
        "            rows = financial_table.find_all(\"tr\")\n",
        "            for row in rows:\n",
        "                cols = row.find_all(\"td\")\n",
        "                if len(cols) >= 2:  # Ensure it's a key-value row\n",
        "                    key = cols[0].get_text(strip=True)\n",
        "                    value = cols[1].get_text(strip=True)\n",
        "                    details[key] = value\n",
        "\n",
        "        # Extract metrics\n",
        "        metrics_table = soup.select_one(\"#summary > table:nth-of-type(1)\")\n",
        "        if metrics_table:\n",
        "            rows = metrics_table.find_all(\"tr\")\n",
        "            for row in rows:\n",
        "                cols = row.find_all(\"td\")\n",
        "                if len(cols) == 2:\n",
        "                    key = cols[0].get_text(strip=True)\n",
        "                    value = cols[1].get_text(strip=True)\n",
        "                    details[key] = value\n",
        "\n",
        "        # Extract movie details\n",
        "        details_table = soup.select_one(\"#summary > table:nth-of-type(3)\")\n",
        "        if not details_table:\n",
        "            details_table = soup.select_one(\"#summary_mobile > div > table:nth-child(11)\")\n",
        "        if not details_table:\n",
        "            details_table = soup.select_one(\"#summary > table:nth-child(11)\")\n",
        "        if not details_table:\n",
        "            details_table = soup.select_one(\"#summary > table:nth-child(13)\")\n",
        "        if not details_table:\n",
        "            details_table = soup.select_one(\"#summary > table:nth-child(9)\")\n",
        "        if not details_table:\n",
        "            details_table = soup.select_one(\"#summary > table:nth-child(6)\")\n",
        "        if not details_table:\n",
        "            details_table = soup.select_one(\"#summary > table:nth-child(8)\")\n",
        "        if not details_table:\n",
        "            details_table = soup.select_one(\"#summary > table:nth-child(4)\")\n",
        "        if not details_table:\n",
        "            details_table = soup.select_one(\"#summary > table:nth-child(10)\")\n",
        "        if not details_table:\n",
        "            details_table = soup.select_one(\"#summary > table:nth-child(12)\")\n",
        "        if not details_table:\n",
        "            details_table = soup.select_one(\"#summary > table:nth-child(7)\")\n",
        "        if not details_table:\n",
        "            details_table = soup.select_one(\"#summary > table:nth-child(5)\")\n",
        "\n",
        "        if details_table:\n",
        "            rows = details_table.find_all(\"tr\")\n",
        "            for row in rows:\n",
        "                cols = row.find_all(\"td\")\n",
        "                if len(cols) == 2:\n",
        "                    key = cols[0].get_text(strip=True)\n",
        "                    value = cols[1].get_text(strip=True)\n",
        "                    details[key] = value\n",
        "\n",
        "        return details\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {movie_url}: {e}\")\n",
        "        return {}\n",
        "\n",
        "# Loop through all budget pages\n",
        "for page in page_numbers:\n",
        "    url = f\"{budget_url}/{page}\" if page != 1 else budget_url\n",
        "    print(f\"Scraping: {url}\")\n",
        "\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        # Wait for the table to load\n",
        "        WebDriverWait(driver, 20).until(\n",
        "            EC.presence_of_element_located((By.TAG_NAME, \"table\"))\n",
        "        )\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "        table = soup.find(\"table\")\n",
        "\n",
        "        # Extract headers (only once)\n",
        "        if df.empty:\n",
        "            headers = [header.text.strip() for header in table.find_all(\"th\")] + [\"Movie URL\"]\n",
        "\n",
        "        # Extract table rows\n",
        "        for row in table.find_all(\"tr\")[1:]:  # Skip header row\n",
        "            cells = row.find_all(\"td\")\n",
        "            row_data = [cell.text.strip() for cell in cells]\n",
        "\n",
        "            # Find movie link inside <b> <a> tag\n",
        "            movie_link_tag = row.find(\"b\").find(\"a\") if row.find(\"b\") else None\n",
        "            movie_link = f\"{base_url}{movie_link_tag['href']}\" if movie_link_tag else \"N/A\"\n",
        "\n",
        "            if row_data:\n",
        "                row_data.append(movie_link)  # Append movie URL\n",
        "\n",
        "                # Scrape extra movie details\n",
        "                if movie_link != \"N/A\":\n",
        "                    extra_details = scrape_movie_details(movie_link)\n",
        "                    extra_details.pop(\"Movie URL\", None)\n",
        "                    row_data.extend(extra_details.values())  # Append extra details dynamically\n",
        "\n",
        "                # Create a temporary DataFrame for the current movie\n",
        "                temp_df = pd.DataFrame([row_data], columns=headers + list(extra_details.keys()))\n",
        "\n",
        "                # Append the temporary DataFrame to the main DataFrame\n",
        "                df = pd.concat([df, temp_df], ignore_index=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "        continue\n",
        "\n",
        "# Close browser\n",
        "driver.quit()\n",
        "\n",
        "# Save to CSV\n",
        "csv_path = \"combined_movie_budgets.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(f\"Scraping completed! CSV file saved: {csv_path}\")"
      ],
      "metadata": {
        "id": "gRvZj1R2Ir1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Dv9R7CrZlz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rescraping the missing values\n",
        "\n",
        "# Function to get a random user-agent\n",
        "def get_random_user_agent():\n",
        "    ua = UserAgent()\n",
        "    return ua.random\n",
        "# Setup WebDriver with enhanced options\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "chrome_options.add_argument(f\"user-agent={get_random_user_agent()}\")\n",
        "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "chrome_options.add_experimental_option(\"useAutomationExtension\", False)\n",
        "\n",
        "service = Service(ChromeDriverManager().install())\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "driver.set_page_load_timeout(120)  # Set page load timeout to 120 seconds\n",
        "\n",
        "# Function to scrape extra movie details\n",
        "def scrape_movie_details(movie_url):\n",
        "    try:\n",
        "        driver.get(movie_url)\n",
        "        # Wait for the page to load\n",
        "        WebDriverWait(driver, 120).until(\n",
        "            EC.presence_of_element_located((By.CSS_SELECTOR, \"#movie_finances\"))\n",
        "        )\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "        details = {\"Movie URL\": movie_url}\n",
        "\n",
        "        # Extract financial information\n",
        "        financial_table = soup.find(\"table\", id=\"movie_finances\")\n",
        "        if financial_table:\n",
        "            rows = financial_table.find_all(\"tr\")\n",
        "            for row in rows:\n",
        "                cols = row.find_all(\"td\")\n",
        "                if len(cols) >= 2:  # Ensure it's a key-value row\n",
        "                    key = cols[0].get_text(strip=True)\n",
        "                    value = cols[1].get_text(strip=True)\n",
        "                    details[key] = value\n",
        "\n",
        "        # Extract metrics\n",
        "        metrics_table = soup.select_one(\"#summary > table:nth-of-type(1)\")\n",
        "        if metrics_table:\n",
        "            rows = metrics_table.find_all(\"tr\")\n",
        "            for row in rows:\n",
        "                cols = row.find_all(\"td\")\n",
        "                if len(cols) == 2:\n",
        "                    key = cols[0].get_text(strip=True)\n",
        "                    value = cols[1].get_text(strip=True)\n",
        "                    details[key] = value\n",
        "\n",
        "        # Extract movie details\n",
        "        details_table = soup.select_one(\"#summary > table:nth-of-type(3)\")\n",
        "        if not details_table:\n",
        "            details_table = soup.select_one(\"#summary_mobile > div > table:nth-child(11)\")\n",
        "        if not details_table:\n",
        "            details_table = soup.select_one(\"#summary > table:nth-child(11)\")\n",
        "        if not details_table:\n",
        "            details_table = soup.select_one(\"#summary > table:nth-child(13)\")\n",
        "        if not details_table:\n",
        "            details_table = soup.select_one(\"#summary > table:nth-child(9)\")\n",
        "        if not details_table:\n",
        "            details_table = soup.select_one(\"#summary > table:nth-child(6)\")\n",
        "        if not details_table:\n",
        "            details_table = soup.select_one(\"#summary > table:nth-child(8)\")\n",
        "        if not details_table:\n",
        "            details_table = soup.select_one(\"#summary > table:nth-child(4)\")\n",
        "        if not details_table:\n",
        "            details_table = soup.select_one(\"#summary > table:nth-child(10)\")\n",
        "        if not details_table:\n",
        "            details_table = soup.select_one(\"#summary > table:nth-child(12)\")\n",
        "        if not details_table:\n",
        "            details_table = soup.select_one(\"#summary > table:nth-child(7)\")\n",
        "        if not details_table:\n",
        "            details_table = soup.select_one(\"#summary > table:nth-child(5)\")\n",
        "\n",
        "        if details_table:\n",
        "            rows = details_table.find_all(\"tr\")\n",
        "            for row in rows:\n",
        "                cols = row.find_all(\"td\")\n",
        "                if len(cols) == 2:\n",
        "                    key = cols[0].get_text(strip=True)\n",
        "                    value = cols[1].get_text(strip=True)\n",
        "                    details[key] = value\n",
        "\n",
        "        return details\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {movie_url}: {e}\")\n",
        "        return {}\n",
        "\n",
        "# Load the existing CSV file\n",
        "csv_path = \"combined_movie_budgets.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Identify rows with missing 'Genre' values\n",
        "missing_genre_rows = df[df['Genre:'].isna()]\n",
        "\n",
        "# Loop through rows with missing 'Genre' and rescrape\n",
        "for index, row in missing_genre_rows.iterrows():\n",
        "    movie_url = row['Movie URL']\n",
        "    if movie_url != \"N/A\":\n",
        "        print(f\"Rescraping: {movie_url}\")\n",
        "        extra_details = scrape_movie_details(movie_url)\n",
        "\n",
        "        # Update all columns in the DataFrame with the newly scraped data\n",
        "        for column in df.columns:\n",
        "            if column in extra_details:\n",
        "                df.at[index, column] = extra_details[column]\n",
        "\n",
        "# Close browser\n",
        "driver.quit()\n",
        "\n",
        "# Save the updated DataFrame back to the CSV file\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(f\"Rescraping completed! CSV file updated: {csv_path}\")"
      ],
      "metadata": {
        "id": "opzKeLMqZmIz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}